(1)假设检验

(2)T检验法

(3)一元回归

(4)lm()线性模型函数
***适用于多元线性模型的基本函数式lm()，其调用形式是：
***fitted.model<-lm(formula,data=data.frrame)
***其中formula为模型公式，data.frame为数据框，返回值为线性结果的对象存放在fitted.model中
***fm2<-lm(y~x1+x2,data=production)
***使用于有关于x1和x2的多元回归模型(隐含着截距项)
***y~1+x或者y~x均表示y=a+bx有截距式的线性模型
***通过原点的线性模型可以表达为：y~x-1或y~x+0或y~0+x

(5)计算残差平方和
deviance(a)

(6)计算残差
residuals(a)

(7)多元线性回归
预测
new<-data.frame(x1=80,x2=40)
lm.pred<-predict(lm.sol,new,interval="prediction",level=0.95)#lm.sol公式
lm.pred#输出lm.pred

例子：iris数据集，研究花瓣与花萼的长度、宽度之间的联系
准备数据：
x=iris[which(iris$Species=="setosa"),1:4]
plot(x)#画出散点图

(8)瑞士数据集
s=lm(Fertility~.,data=swiss)#lm()回归函数，Fertility因变量，~关系，"."除因变量外所有变量
print(s)
summary(s)

(9)逐步回归
1.向前引入法：从一元回归开始，逐步增加变量，使指数值达到最优
2.向后剔除法：从全变量方程开始，逐步删去某个变量，使指标值达到最优为止
3.逐步筛选法：综合上述两种方式

(10)多元线性回归的核心问题
1.RSS(残差平方和/残差标准差)与相关系数平方选择法：遍历所有可能的组合，选出RSS最小，R^2最大的模型
***summary(a)
***Residual standard error的值越小模型越准确
***Multiple R-squard(相关系数平方R方)的值越大模型越准确
2.AIC(akaike information criterion)准则与BIC(bayesian information criterion)准则
***AIC=n ln(RRS/n)+2p
***n为变量总个数，p为选出变量的个数，AIC越小越好

(11)step()函数
sl=step(s,direction="backward")#s表示线性回归模型，direction="forward"向前引入法/"backward"向后剔除法/"both"逐步筛选法
以瑞士数据集为例子
s=lm(Fertility~.,data=swiss)#最初的线性回归方程
优化线性回归方程
sl=step(s,direction="backward")

(12)多元线性回归（手动调整变量）
add1()增加一个变量
drop1()去除一个变量